{"cells":[{"cell_type":"markdown","source":["## Exercise: Select Optimal Model by Tuning Hyperparameters\n\nUse grid search and cross-validation to tune the hyperparameters from a logistic regression model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce53d4c2-1de2-40c5-bf47-5f10e4032747"}}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3b7d5c5-efbb-4d51-8499-43ef215748fd"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da77a282-530b-46d3-a5ce-3ee82a68a1e9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 1: Import the Data\n\nImport the data and perform a train/test split."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b46d737-9f3c-40f4-9aeb-2ae83c444510"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ncols = [\"index\",\n \"sample-code-number\",\n \"clump-thickness\",\n \"uniformity-of-cell-size\",\n \"uniformity-of-cell-shape\",\n \"marginal-adhesion\",\n \"single-epithelial-cell-size\",\n \"bare-nuclei\",\n \"bland-chromatin\",\n \"normal-nucleoli\",\n \"mitoses\",\n \"class\"]\n\ncancerDF = (spark.read  # read the data\n  .option(\"HEADER\", True)\n  .option(\"inferSchema\", True)\n  .csv(\"/mnt/training/cancer/biopsy/biopsy.csv\")\n)\n\ncancerDF = (cancerDF    # Add column names and drop nulls\n  .toDF(*cols)\n  .withColumn(\"bare-nuclei\", col(\"bare-nuclei\").isNotNull().cast(\"integer\"))\n)\n\ndisplay(cancerDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02cd709f-3b12-4f30-a40d-8964a107671f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Perform a train/test split to create `trainCancerDF` and `testCancerDF`.  Put 80% of the data in `trainCancerDF` and use the seed that is set for you."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"caddf120-2755-41d9-bbb4-15b9cb657733"}}},{"cell_type":"code","source":["# TODO\nseed = 42\ntrainCancerDF, testCancerDF = cancerDF.randomSplit([0.8, 0.2], seed=seed)\n\ndisplay(trainCancerDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea05acba-013e-4e93-be50-377674b449b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: Create a Pipeline\n\nCreate a pipeline `cancerPipeline` that consists of the following stages:<br>\n\n1. `indexer`: a `StringIndexer` that takes `class` as an input and outputs the column `is-malignant`\n2. `assembler`: a `VectorAssembler` that takes all of the other columns as an input and outputs  the column `features`\n3. `logr`: a `LogisticRegression` that takes `features` as the input and `is-malignant` as the output variable"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f8ceff-ce7a-446a-915e-1f980ce72f2d"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"is-malignant\")\nassembler = VectorAssembler(inputCols=cols[2:-1], outputCol=\"features\")\nlogr = LogisticRegression(featuresCol=\"features\", labelCol=\"is-malignant\")\n\ncancerPipeline = Pipeline(stages = [indexer, assembler, logr])\n\n# logrModel = cancerPipeline.fit(trainCancerDF) # To fit without cross-validation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a817c86-62a4-481a-b018-c1c1c00fda1a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\ndbTest(\"ML1-P-08-02-01\", True, type(indexer) == type(StringIndexer()))\ndbTest(\"ML1-P-08-02-02\", True, indexer.getInputCol() == 'class')\ndbTest(\"ML1-P-08-02-03\", True, indexer.getOutputCol() == 'is-malignant')\n\ndbTest(\"ML1-P-08-02-04\", True, type(assembler) == type(VectorAssembler()))\ndbTest(\"ML1-P-08-02-05\", True, assembler.getInputCols() == cols[2:-1])\ndbTest(\"ML1-P-08-02-06\", True, assembler.getOutputCol() == 'features')\n\ndbTest(\"ML1-P-08-02-07\", True, type(logr) == type(LogisticRegression()))\ndbTest(\"ML1-P-08-02-08\", True, logr.getLabelCol() == \"is-malignant\")\ndbTest(\"ML1-P-08-02-09\", True, logr.getFeaturesCol() == 'features')\n\ndbTest(\"ML1-P-08-02-10\", True, type(cancerPipeline) == type(Pipeline()))\n\nprint(\"Tests passed!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a762002-9965-4e3e-bee8-2652d4b83356"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 3: Create Grid Search Parameters\n\nTake a look at the parameters for our `LogisticRegression` object.  Use this to build the inputs to grid search."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5162ae6d-572d-4b4e-9c87-2a2ad71b0888"}}},{"cell_type":"code","source":["print(logr.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cd383b9-81c0-45f1-9dc2-6577d87db1c0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Create a `ParamGridBuilder` object with two grids:<br><br>\n\n1. A regularization parameter `regParam` of `[0., .2, .8, 1.]`\n2. Test both with and without an intercept using `fitIntercept`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80e3cc23-220b-42bd-bcb9-808481f77e06"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml.tuning import ParamGridBuilder\n\ncancerParamGrid = (ParamGridBuilder()\n  .addGrid(logr.regParam, [0., .2, .8, 1.])\n  .addGrid(logr.fitIntercept, [True, False])\n  .build()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fefb318c-aad6-49f3-9cdf-d35c2f52eada"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ML1-P-08-03-01\", True, type(cancerParamGrid) == list)\n\nprint(\"Tests passed!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b41a4b1-293e-4dae-852e-f73ee0fbbd5a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 4: Perform 3-Fold Cross-Validation\n\nCreate a `BinaryClassificationEvaluator` object and use it to perform 3-fold cross-validation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3612f4c3-67a8-412e-b136-fb3404754fd7"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nbinaryEvaluator = BinaryClassificationEvaluator(\n  labelCol = \"is-malignant\", \n  metricName = \"areaUnderROC\"\n)\n\ncancerCV = CrossValidator(\n  estimator = cancerPipeline,             # Estimator (individual model or pipeline)\n  estimatorParamMaps = cancerParamGrid,   # Grid of parameters to try (grid search)\n  evaluator=binaryEvaluator,              # Evaluator\n  numFolds = 3,                           # Set k to 3\n  seed = 42                               # Seed to sure our results are the same if ran again\n)\n\ncancerCVModel = cancerCV.fit(trainCancerDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba75ec9a-f538-4201-b17f-f7a2a75ecba4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\ndbTest(\"ML1-P-08-04-01\", True, type(binaryEvaluator) == type(BinaryClassificationEvaluator()))\ndbTest(\"ML1-P-08-04-02\", True, type(cancerCV) == type(CrossValidator()))\n\nprint(\"Tests passed!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ab415fc-35ff-4047-bc21-ea8393cfc386"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 5: Examine the results\n\nTake a look at the results.  Which combination of hyperparameters learned the most from the data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"175d060b-bde4-4b45-b62a-674b78c396cc"}}},{"cell_type":"code","source":["for params, score in zip(cancerCVModel.getEstimatorParamMaps(), cancerCVModel.avgMetrics):\n  print(\"\".join([param.name+\"\\t\"+str(params[param])+\"\\t\" for param in params]))\n  print(\"\\tScore: {}\".format(score))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75d0721d-6f27-4447-bb3a-1a83a0d1adbd"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2. Exercise Model Selection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2266511824507798}},"nbformat":4,"nbformat_minor":0}
